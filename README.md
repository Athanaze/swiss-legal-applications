# swiss-legal-applications

Useful resources regarding legal applications.

## laws.json

Contains all the federal laws. fileUrl.value contains the link for the xml version of each law.
The uuid has been generated by us, just to give a unique identifier to each of them. It has no connection with other identification methods.

## Websites

[bger.ch](https://www.bger.ch/)

- Build queries and call via CURL

  https://fedlex.data.admin.ch/en-CH/sparql

- Nice structure with unique, ascending number (just leave the /2024 at the end no matter what). The earliest we got working is 30000

  https://droitpourlapratique.ch/decision/90584/2024
  
  All the content is within ```<div id="print_main">```

  https://droitpourlapratique.ch/decision/90584/2024
  
---
## AI tools

### Langchain script

Extract the xml articles (also referenced as "article"  from a legal point of view) from a local xml file and embeds them in a chroma database.


/!\ Splits the articles that are too big in the middle, with no precautions /!\

Requires the user to press enter to launch the embedding (in vscode this opens up a pop up on top)

```python

import xml.etree.ElementTree as ET
import tiktoken
import random
import matplotlib.pyplot as plt
import os
import getpass
from langchain_core.documents.base import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

os.environ['OPENAI_API_KEY'] = "<key here or just a environment variable>"

def chroma_db_from_xml_file(xml_file, chroma_dir_name, metadata, add_to_existing=False):
    MAX_T = 8192

    def extract_articles():
        tree = ET.parse(xml_file)
        root = tree.getroot()
        articles = []
        
        def find_articles(element):
            if "article" in element.tag:
                articles.append(ET.tostring(element, encoding='unicode'))
            for child in element:
                find_articles(child)
        
        find_articles(root)
        return articles


    def split_if_necessary(string_arr):
        enc = tiktoken.get_encoding("cl100k_base")
        new_string_arr = []
        for e in string_arr:
            if len(enc.encode(e)) > MAX_T:
                mid = len(e) // 2
                a, b = e[:mid], e[mid:]
                new_string_arr.append(a)
                new_string_arr.append(b)
            else:
                new_string_arr.append(e)
        print(len(string_arr), len(new_string_arr))
        
        rerun_algo = False
        for s in new_string_arr:
            if len(enc.encode(e)) > MAX_T:
                rerun_algo = True
        
        if rerun_algo:
            return split_if_necessary(new_string_arr)
        
        return new_string_arr

    def get_token_counts(string_array):
        token_counts = []
        enc = tiktoken.get_encoding("cl100k_base")
        for string in string_array:
            t = len(enc.encode(string))
            if t > MAX_T:
                print(t)
            token_counts.append(t)

        return token_counts
    def display_token_count_chart(string_array):
        token_counts = get_token_counts(string_array)

        plt.figure(figsize=(10, 30))
        plt.bar(range(len(string_array)), token_counts)
        plt.xlabel('String Index')
        plt.ylabel('Token Count')
        plt.title('Token Count Distribution')
        
        # Add a horizontal red line at 8192
        plt.axhline(y=MAX_T, color='red', linestyle='-', linewidth=1)
        
        plt.show()

    articles = extract_articles()
    s_a = split_if_necessary(articles)
    display_token_count_chart(s_a)
    COST_PER_1M_TOKEN = 0.13
    n_tokens = sum(get_token_counts(s_a))
    input(f"N TOKENS {n_tokens}\nPRICE USD {COST_PER_1M_TOKEN*(n_tokens/1000000)}")
    
    documents = []
    for l in s_a:
        doc = Document(page_content = l)
        doc.metadata=metadata
        documents.append(doc)

    if(add_to_existing):
        db = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=chroma_dir_name)
        db.add_documents(documents)
        return db
    
    return Chroma.from_documents(documents, OpenAIEmbeddings(), persist_directory=chroma_dir_name)
```

Somehow hard to find via google, so here it is:
[Document class documentation](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document)


---



---

### Created and maintained by [Liechti Consulting](https://liechticonsulting.com) &nbsp; :switzerland:

Need AI-powered solutions? Check out [gptprepay.com](https://gptprepay.com), a Liechti Consulting product for custom GPT services! &nbsp; :rocket:

---
